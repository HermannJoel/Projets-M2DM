remove.packages("ModelBased")
library(ModelBased)
library(dplyr)
library(readr)
library(readxl)
library(keras)
library(tensorflow)
library(rsample)
#import the train and test csv files
train<-read_csv(choose.files(), col_names = FALSE)
test<-read_csv(choose.files(), col_names = FALSE)
set.seed(007)
sizetrain<-1400
sizetest<-600
spletrain<-train[sample(x=1:nrow(train), size=sizetrain, replace=FALSE), ]
dim(spletrain)
spletest<-test[sample(x=1:nrow(test), size=sizetest, replace=FALSE), ]
dim(spletrain)
spletrain<-mutate_at(spletrain, vars(X1), as.factor)
spletest<-mutate_at(spletest, vars(X1), as.factor)
summary(spletrain$X1)
min(spletrain[2:785])
max(spletrain[2:785])
#To normalize data between 0&1
div<-function(x){x/255.0}
spletrain[2:785]<-lapply(spletrain[2:785],div)
spletest[2:785]<-lapply(spletest[2:785],div)
min(spletrain[2:785])
max(spletrain[2:785])
#-----------------------------------
#1.
X<-spletrain[2:785]
dim(X)
z<-spletrain[1]
dim(z)
model1<-gmmClassif.EM(X = X, z = z, K = 3)
model1<-gmmClassif.EM(X = X, z = z, K = 2)
set.seed(007)
sizetrain<-500
sizetest<-150
spletrain<-train[sample(x=1:nrow(train), size=sizetrain, replace=FALSE), ]
dim(spletrain)
spletest<-test[sample(x=1:nrow(test), size=sizetest, replace=FALSE), ]
dim(spletrain)
#To convert target into a factor
spletrain<-mutate_at(spletrain, vars(X1), as.factor)
spletest<-mutate_at(spletest, vars(X1), as.factor)
summary(spletrain$X1)
min(spletrain[2:785])
max(spletrain[2:785])
#To normalize data between 0&1
div<-function(x){x/255.0}
spletrain[2:785]<-lapply(spletrain[2:785],div)
spletest[2:785]<-lapply(spletest[2:785],div)
min(spletrain[2:785])
max(spletrain[2:785])
#-----------------------------------
#1.
X<-spletrain[2:785]
z<-spletrain[1]
dim(z)
model1<-gmmClassif.EM(X = X, z = z, K = 2)
rm(list = ls())
#---------Pretraitement------------
data('iris')
library(rsample)
set.seed(007)
split<-initial_split(iris, prop=.70, strata="Species")
iris_train<-training(split)
iris_test<-testing(split)
X_train<-iris_train[,1:4]
y_train<-iris_train[, 5]
X_test<-iris_test[, 1:4]
y_test<-iris_test[, 5]
iris.mclust<-Mclust(X_train)
summary(iris.mclust)
library(mclust)
library(caret)
iris.mclust<-Mclust(X_train)
summary(iris.mclust)
##log-likelihood=152.7456
summary(iris.mclust$BIC)
mclust_cm<-table(y_train, iris.mclust$classification)
print(mclust_cm)
#Accuracy
mclust_acc<-sum(diag(mclust_cm))/sum(mclust_cm)
print(mclust_acc)
#Taux d'erreurs
mclust_err<- 1-mclust_acc
print(mclust_err)
#recall(rappel)
mclust_recall<-diag(mclust_cm)/rowSums(mclust_cm)
#Precision (f-1)
mclust_prec<-diag(mclust_cm)/colSums(mclust_cm)
print(mclust_prec)
caret::confusionMatrix(data=iris.mclust$classification, reference=y_train)
print(mclust_prec)
#Pretraitement des données mnist
rm(list = ls())
getwd()
setwd("C:/Users/nherm/Downloads/VaR/VaR")
getwd()
getwd()
setwd("C:/Users/nherm/Downloads")
getwd()
set.seed(20201130)
n <- 200
coefs0 <- matrix(runif(n * 4, -5, 5), nrow = n) # on tire 4 coef par fonct
x0     <- seq(0, 1, length.out = 1024)          # grille d'éval pour chaq fonct
X      <- poly(x0, degree = 4)
datos <- coefs0 %*% t(X)                        # données
matplot(t(datos), type = "l", lty = 1, col = 1)
## ___a. Determiner le nombre de variables et des observations ####
ncol(datos)
nrow(datos)
## ___a. Determiner le nombre de variables et des observations ####
ncol(datos)#nombre de variables
nrow(datos)#nombre d'observations
## ___b. Estimer la dimension par ACP (normée) ####
datos.pca <- prcomp(datos, center=T, scale. = T)
plot(datos.pca$sdev)
## ___c. Idem pour la correlation dimension ####
library(pdist)
# fonction qui détermine la corrélation dimension pour plusieurs Epsilon
corrDim <- function(data, epsilon = 10^seq(-2, 1, length.out = 100)){
n <- nrow(data)
lEps <- length(epsilon)
C <- rep(0,lEps)
distances <- as.matrix(pdist(data,data))
for(k in 1:lEps){
C[k] <- sum(distances<epsilon[k]) /n / (n-1)
}
return.df <- data.frame(C, epsilon)
return.df$epsilon <- epsilon
return.df$C <- C
return(return.df)
}
# Fonction pour la dérivée
derivate <- function(x, y) {
ll     <- length(y)
deltax <- x[2] - x[1]
deltaf <- y[3:ll] - y[1:(ll - 2)]
return(c(NA, deltaf / 2 / deltax, NA))
}
corrDim <- corrDim(datos)
## ___b. Estimer la dimension par ACP (normée) ####
datos.pca <- prcomp(datos, center=T, scale. = T)
plot(datos.pca$center)
(datos.pca$x)
iris<- data("iris")
iris.pca <- prcomp(iris[,-5], center=T, scale. = T)
plot(iris.pca$sdev)
iris.pca <- prcomp(iris[,-5], center=T, scale. = T)
data("iris")
iris.pca <- prcomp(iris[,-5], center=T, scale. = T)
plot(iris.pca$sdev)
rm(list = ls())
set.seed(20201130)
n <- 200
coefs0 <- matrix(runif(n * 4, -5, 5), nrow = n) # on tire 4 coef par fonct
x0     <- seq(0, 1, length.out = 1024)          # grille d'éval pour chaq fonct
X      <- poly(x0, degree = 4)
datos <- coefs0 %*% t(X)                        # données
matplot(t(datos), type = "l", lty = 1, col = 1) #plot
## ___a. Determiner le nombre de variables et des observations ####
ncol(datos)
#nombre de variables=1024
nrow(datos)
## ___b. Estimer la dimension par ACP (normée) ####
datos.pca <- prcomp(datos, center=T, scale. = T)
plot(datos.pca$sdev)
#estimation de la densité à 0
## ___c. Idem pour la correlation dimension ####
library(pdist)
# fonction qui détermine la corrélation dimension pour plusieurs Epsilon
corrDim <- function(data, epsilon = 10^seq(-2, 1, length.out = 100)){
n <- nrow(data)
lEps <- length(epsilon)
C <- rep(0,lEps)
distances <- as.matrix(pdist(data,data))
for(k in 1:lEps){
C[k] <- sum(distances<epsilon[k]) /n / (n-1)
}
return.df <- data.frame(C, epsilon)
return.df$epsilon <- epsilon
return.df$C <- C
return(return.df)
}
datos.pca <- prcomp(datos, center=T, scale. = F)
plot(datos.pca$sdev)
set.seed(20201130)
n <- 200
coefs0 <- matrix(runif(n * 4, -5, 5), nrow = n) # on tire 4 coef par fonct
x0     <- seq(0, 1, length.out = 1024)          # grille d'éval pour chaq fonct
X      <- poly(x0, degree = 4)
datos <- coefs0 %*% t(X)                        # données
matplot(t(datos), type = "l", lty = 1, col = 1) #plot
datos.pca <- prcomp(datos, center=F, scale. = F)
plot(datos.pca$sdev)
datos.pca <- prcomp(datos, center=T, scale. = T)
plot(datos.pca$scale)
plot(datos.pca$rotation)
plot(datos.pca$sdev)
corrDim <- corrDim(datos)
plot(log10(corrDim$epsilon),
derivate(log10(corrDim$epsilon), log10(corrDim$C)),
type = 'l', xlim = c(-2,1))
return(c(NA, deltaf / 2 / deltax, NA))
# Fonction pour la dérivée
derivate <- function(x, y) {
ll     <- length(y)
deltax <- x[2] - x[1]
deltaf <- y[3:ll] - y[1:(ll - 2)]
return(c(NA, deltaf / 2 / deltax, NA))
}
plot(log10(corrDim$epsilon),
derivate(log10(corrDim$epsilon), log10(corrDim$C)),
type = 'l', xlim = c(-2,1))
## ___d. Idem pour un ACP à noyau (utiliser le noyau gaussien) ####
library(kernlab)
datos_kpca <- kpca(as.matrix(datos), kernel="rbfdot", kpar =
list(sigma = 0.1), features = 0, th = 1e-4, na.action = na.omit)
plot(eig(datos_kpca),  xlim = c(0,10))
#un coude apparaît à environ 6
## ___e. Obtenir un MDS classique ####
## Utiliser l'estimation de la dimension la plus pertinente parmi celle
## calculeés.
#k=6
datos_mds <- cmdscale(dist(datos), k = 6, eig = FALSE, add = FALSE,
x.ret = T)
datos_mds
plot(datos_mds$x)
## ___f. Obtenir un LLE ####
## Utiliser l'estimation de la dimension la plus pertinente parmi celle
## calculeés.
library(lle)
calc_k(datos, 6)
data("iris")
calc_k(iris[,-5], 2)
calc_k(datos, 6)
calc_k(datos, 2) #k=7
datos.pca <- prcomp(datos, center=T, scale. = T)
plot(datos.pca$sdev)
iris.pca <- prcomp(iris[,-5], center=T, scale. = T)
plot(iris.pca$sdev)
corrDim <- corrDim(datos)
#estimation de la densité à 6
## ___c. Idem pour la correlation dimension ####
library(pdist)
# fonction qui détermine la corrélation dimension pour plusieurs Epsilon
corrDim <- function(data, epsilon = 10^seq(-2, 1, length.out = 100)){
n <- nrow(data)
lEps <- length(epsilon)
C <- rep(0,lEps)
distances <- as.matrix(pdist(data,data))
for(k in 1:lEps){
C[k] <- sum(distances<epsilon[k]) /n / (n-1)
}
return.df <- data.frame(C, epsilon)
return.df$epsilon <- epsilon
return.df$C <- C
return(return.df)
}
# Fonction pour la dérivée
derivate <- function(x, y) {
ll     <- length(y)
deltax <- x[2] - x[1]
deltaf <- y[3:ll] - y[1:(ll - 2)]
return(c(NA, deltaf / 2 / deltax, NA))
}
corrDim <- corrDim(datos)
plot(log10(corrDim$epsilon),
derivate(log10(corrDim$epsilon), log10(corrDim$C)),
type = 'l', xlim = c(-2,1))
corrDim <- corrDim(iris[,-5])
corrDim <- corrDim(iris[,-5])
# Fonction pour la dérivée
derivate <- function(x, y) {
ll     <- length(y)
deltax <- x[2] - x[1]
deltaf <- y[3:ll] - y[1:(ll - 2)]
return(c(NA, deltaf / 2 / deltax, NA))
}
# Fonction pour la dérivée
derivate <- function(x, y) {
ll     <- length(y)
deltax <- x[2] - x[1]
deltaf <- y[3:ll] - y[1:(ll - 2)]
return(c(NA, deltaf / 2 / deltax, NA))
}
corrDim <- corrDim(iris[,-5])
# fonction qui détermine la corrélation dimension pour plusieurs Epsilon
corrDim <- function(data, epsilon = 10^seq(-2, 1, length.out = 100)){
n <- nrow(data)
lEps <- length(epsilon)
C <- rep(0,lEps)
distances <- as.matrix(pdist(data,data))
for(k in 1:lEps){
C[k] <- sum(distances<epsilon[k]) /n / (n-1)
}
return.df <- data.frame(C, epsilon)
return.df$epsilon <- epsilon
return.df$C <- C
return(return.df)
}
corrDim <- corrDim(iris[,-5])
plot(log10(corrDim$epsilon),
derivate(log10(corrDim$epsilon), log10(corrDim$C)),
type = 'l', xlim = c(-2,1))
# Fonction pour la dérivée
derivate <- function(x, y) {
ll     <- length(y)
deltax <- x[2] - x[1]
deltaf <- y[3:ll] - y[1:(ll - 2)]
return(c(NA, deltaf / 2 / deltax, NA))
}
corrDim <- corrDim(datos)
# fonction qui détermine la corrélation dimension pour plusieurs Epsilon
corrDim <- function(data, epsilon = 10^seq(-2, 1, length.out = 100)){
n <- nrow(data)
lEps <- length(epsilon)
C <- rep(0,lEps)
distances <- as.matrix(pdist(data,data))
for(k in 1:lEps){
C[k] <- sum(distances<epsilon[k]) /n / (n-1)
}
return.df <- data.frame(C, epsilon)
return.df$epsilon <- epsilon
return.df$C <- C
return(return.df)
}
corrDim <- corrDim(datos)
plot(log10(corrDim$epsilon),
derivate(log10(corrDim$epsilon), log10(corrDim$C)),
type = 'l', xlim = c(-2,1))
iris.kpca <- kpca(as.matrix(iris[,-5]), kernel="rbfdot", kpar =
list(sigma = 0.1), features = 0, th = 1e-4, na.action = na.omit)
plot(eig(iris.kpca),  xlim = c(0,10))
datos_kpca <- kpca(as.matrix(datos), kernel="rbfdot", kpar =
list(sigma = 0.1), features = 0, th = 1e-4, na.action = na.omit)
plot(eig(datos_kpca),  xlim = c(0,10))
## ___e. Obtenir un MDS classique ####
## Utiliser l'estimation de la dimension la plus pertinente parmi celle
## calculeés.
#Considerons 4 comme la dimension la plus pertinante; k=4
datos_mds <- cmdscale(dist(datos), k = 4, eig = FALSE, add = FALSE,
x.ret = T)
plot(datos_mds$x)
## ___f. Obtenir un LLE ####
## Utiliser l'estimation de la dimension la plus pertinente parmi celle
## calculeés.
library(lle)
calc_k(datos, 2) #k=7
#Considerons 4 comme la dimension la plus pertinante
calc_k(datos, 4)
#Considerons 3 comme la dimension la plus pertinante
calc_k(datos, 3)#K=7
datos_lle <-  lle(datos, m = 3, k = 7)
datos_lle$choise
rm(list = ("cls"))
rm(list =(cls))
rm(list =(cls))
#Pretraitement des données mnist
rm(list = ls())
rm(list =ls())
getwd()
setwd("C:/Users/nherm/Downloads/VaR/VaR")
getwd()
